{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e073e55-68b9-4b8b-9aa2-64916b686ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from o3d_tools.visualize import PointCloudProject\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Dataset, DataLoader\n",
    "import torch_cluster\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import PointTransformerConv, global_max_pool, knn_graph\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm \n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73aff1bf-9bc1-41cc-87b0-977191aa6610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a25f71d-90ad-4b5c-9dba-f89b49243b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project1 = PointCloudProject(project='Project1') ; project2 = PointCloudProject(project='Project2')\n",
    "project3 = PointCloudProject(project='Project3') ; project4 = PointCloudProject(project='Project4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cfa0b6-819f-4ece-acd8-b0377876ed2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of objects in Project 1: ['Structural_ColumnBeam', 'HVAC_Duct', 'Pipe', 'Structural_IBeam']\n",
      "Types of objects in Project 2: ['Structural_ColumnBeam', 'Pipe', 'Structural_IBeam']\n",
      "Types of objects in Project 3: ['Structural_ColumnBeam']\n",
      "Types of objects in Project 4: ['Structural_IBeam']\n",
      "Initial preprocessing done\n",
      "Train val test split done\n",
      "Label encoding done\n"
     ]
    }
   ],
   "source": [
    "def read_and_preprocess_data():\n",
    "\n",
    "    objs1 = project1.objects_df\n",
    "\n",
    "    print(f\"Types of objects in Project 1: {list(objs1.keys())}\")\n",
    "    pipes1 = objs1[\"Pipe\"]\n",
    "    scbs1 = objs1[\"Structural_ColumnBeam\"]\n",
    "    sibs1 = objs1[\"Structural_IBeam\"]\n",
    "    hvducts = objs1[\"HVAC_Duct\"]\n",
    "\n",
    "    objs2 = project2.objects_df\n",
    "\n",
    "    print(f\"Types of objects in Project 2: {list(objs2.keys())}\")\n",
    "    pipes2 = objs2[\"Pipe\"]\n",
    "    scbs2 = objs2[\"Structural_ColumnBeam\"]\n",
    "    sibs2 = objs2[\"Structural_IBeam\"]\n",
    "\n",
    "    objs3 = project3.objects_df\n",
    "\n",
    "    print(f\"Types of objects in Project 3: {list(objs3.keys())}\")\n",
    "    scbs3 = objs3[\"Structural_ColumnBeam\"]\n",
    "\n",
    "    objs4 = project4.objects_df\n",
    "\n",
    "    print(f\"Types of objects in Project 4: {list(objs4.keys())}\")\n",
    "    sibs4 = objs4[\"Structural_IBeam\"]\n",
    "\n",
    "\n",
    "    boxes1 = pd.concat([scbs1, hvducts, pipes1, sibs1])\n",
    "    boxes2 = pd.concat([scbs2, pipes2, sibs2])\n",
    "    boxes3 = scbs3\n",
    "    boxes4 = sibs4\n",
    "\n",
    "    boxes = [boxes1, boxes2, boxes3, boxes4]\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        boxes[i].rename(columns={\" Label\": \"label\", \" BB.Min.X \" : \"min_x\", \" BB.Min.Y \" : \"min_y\", \" BB.Min.Z \" : \"min_z\",\n",
    "                             \" BB.Max.X \" : \"max_x\", \" BB.Max.Y \" : \"max_y\", \" BB.Max.Z\" : \"max_z\"}, inplace=True)\n",
    "\n",
    "    box_data = pd.concat(boxes)\n",
    "\n",
    "    ids_to_points = {}\n",
    "\n",
    "    projects = [project1, project2, project3, project4]\n",
    "\n",
    "    for proj_id, box in enumerate(boxes):\n",
    "\n",
    "        for i, ID in enumerate(box[\"ID\"]):\n",
    "\n",
    "            proj = projects[proj_id]\n",
    "\n",
    "            # .pcd.crop() method expects this bounding box object, create it from the raw min max values\n",
    "            bb = o3d.geometry.AxisAlignedBoundingBox((box[[\"min_x\", \"min_y\", \"min_z\"]].iloc[i]).to_numpy(), \n",
    "                                        (box[[\"max_x\", \"max_y\", \"max_z\"]].iloc[i]).to_numpy())\n",
    "\n",
    "            points_of_id = proj.pcd.crop(bb)\n",
    "\n",
    "            # convert the pointcloud object into a numpy array\n",
    "            # join coordinates and color data into a single 6d nparray for each point\n",
    "\n",
    "            points_coords = np.asarray(points_of_id.points)\n",
    "\n",
    "            tmp = np.asarray(points_of_id.colors)[:,0]\n",
    "\n",
    "            points_colors = (np.asarray(points_of_id.colors)[:,0]).reshape(-1,1)\n",
    "\n",
    "            points_arr = np.concatenate((points_coords, points_colors), axis = 1)\n",
    "\n",
    "            ids_to_points[ID] = points_arr\n",
    "\n",
    "    def sample_point_cloud(points, num_points):\n",
    "\n",
    "        if points.shape[0] > num_points:\n",
    "\n",
    "            # Downsample if the point cloud has more points than needed\n",
    "            idx = np.random.choice(points.shape[0], num_points, replace=False)\n",
    "            return points[idx, :]\n",
    "\n",
    "        elif points.shape[0] < num_points:\n",
    "            # Upsample by repeating random points if the point cloud has fewer points than needed\n",
    "            idx = np.random.choice(points.shape[0], num_points - points.shape[0], replace=True)\n",
    "            return np.concatenate([points, points[idx, :]], axis=0)\n",
    "\n",
    "        else:\n",
    "            # Return the point cloud as is if it already has the correct number of points\n",
    "            return points\n",
    "\n",
    "    fixed_num_points = 2048  # The fixed number of points for all point clouds\n",
    "\n",
    "    point_clouds_resampled = {}\n",
    "\n",
    "    for obj_id in ids_to_points.keys():\n",
    "\n",
    "        point_cloud = ids_to_points[obj_id]\n",
    "        resampled_points = sample_point_cloud(point_cloud, fixed_num_points)\n",
    "        point_clouds_resampled[obj_id] = resampled_points\n",
    "        \n",
    "    def extract_bounding_box_features(min_x, max_x, min_y, max_y, min_z, max_z):\n",
    "    \n",
    "        centroid = [(min_x + max_x) / 2, (min_y + max_y) / 2, (min_z + max_z) / 2]\n",
    "        size = [max_x - min_x, max_y - min_y, max_z - min_z]\n",
    "        volume = size[0] * size[1] * size[2]\n",
    "        \n",
    "        return np.array(centroid + size + [volume])\n",
    "    \n",
    "    data = []\n",
    "    for i in range(box_data.shape[0]):\n",
    "\n",
    "        box = box_data.iloc[i]\n",
    "        features = extract_bounding_box_features(box[\"min_x\"], box[\"max_x\"], box[\"min_y\"],\n",
    "                                                 box[\"max_y\"], box[\"min_z\"], box[\"max_z\"])\n",
    "\n",
    "        data.append(features)\n",
    "\n",
    "    feature_df = pd.DataFrame(data, columns=[\"cx\", \"cy\", \"cz\", \"sx\", \"sy\", \"sz\", \"vol\"])\n",
    "    \n",
    "    box_data = box_data.reset_index(drop=True)\n",
    "    \n",
    "    new_box_data = pd.concat([box_data, feature_df], axis=1)\n",
    "    \n",
    "    return point_clouds_resampled, new_box_data\n",
    "\n",
    "points_dict, box_data = read_and_preprocess_data()\n",
    "\n",
    "print(\"Initial preprocessing done\")\n",
    "\n",
    "y = box_data[\"label\"]\n",
    "X = box_data[[\"ID\", \"min_x\", \"min_y\", \"min_z\", \"max_x\", \"max_y\", \"max_z\", \"cx\", \"cy\", \"cz\", \"sx\", \"sy\", \"sz\", \"vol\"]]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(\"Train val test split done\")\n",
    "\n",
    "label_mapping = {\n",
    "    'Structural_ColumnBeam': 0,\n",
    "    'HVAC_Duct': 1,\n",
    "    'Pipe': 2,\n",
    "    'Structural_IBeam': 3\n",
    "}\n",
    "\n",
    "labels_enc = []\n",
    "for row in box_data[\"label\"]:\n",
    "\n",
    "    labels_enc.append(label_mapping[row])\n",
    "\n",
    "box_data[\"label_enc\"] = labels_enc\n",
    "\n",
    "print(\"Label encoding done\")\n",
    "\n",
    "class CustomData(Data):\n",
    "    def __cat_dim__(self, key, value, *args, **kwargs):\n",
    "        if key == 'bbox_features':\n",
    "            return None  # Indicates that bbox_features is a graph-level attribute\n",
    "        else:\n",
    "            return super(CustomData, self).__cat_dim__(key, value, *args, **kwargs)\n",
    "\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if key == 'bbox_features':\n",
    "            return 0  # No increment needed for graph-level attributes\n",
    "        else:\n",
    "            return super(CustomData, self).__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "def create_point_cloud_data(points, bounding_box_features, labels):\n",
    "\n",
    "    x = torch.tensor(points[:,3:], dtype=torch.float)  # Pointcolors\n",
    "    pos = torch.tensor(points[:, :3], dtype=torch.float) # Point coords\n",
    "\n",
    "    bounding_box_features = torch.tensor(bounding_box_features, dtype=torch.float)\n",
    "\n",
    "    # Create the Data object\n",
    "    data = CustomData(x=x, pos=pos, bbox_features=bounding_box_features, y=torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "    return data\n",
    "\n",
    "data_dict = {}\n",
    "features_list = [\"cx\", \"cy\", \"cz\", \"sx\", \"sy\", \"sz\", \"vol\"]\n",
    "\n",
    "for obj_id in points_dict.keys():\n",
    "\n",
    "    curr_label = ((box_data[box_data[\"ID\"] == obj_id].iloc[:,15]).to_numpy()).reshape(1)\n",
    "    curr_features = ((box_data[box_data[\"ID\"] == obj_id].loc[:,features_list]).to_numpy()).reshape(7)\n",
    "    \n",
    "    data_dict[obj_id] = create_point_cloud_data(points_dict[obj_id], curr_features, curr_label)\n",
    "\n",
    "train_data_objs = {}\n",
    "val_data_objs = {}\n",
    "test_data_objs = {}\n",
    "\n",
    "for key in X_train[\"ID\"]:\n",
    "\n",
    "    train_data_objs[key] = data_dict[key]\n",
    "\n",
    "for key in X_val[\"ID\"]:\n",
    "\n",
    "    val_data_objs[key] = data_dict[key]\n",
    "\n",
    "for key in X_test[\"ID\"]:\n",
    "\n",
    "    test_data_objs[key] = data_dict[key]\n",
    "\n",
    "\n",
    "train_data_arr = []\n",
    "val_data_arr = []\n",
    "test_data_arr = []\n",
    "\n",
    "for key in train_data_objs.keys():\n",
    "\n",
    "    train_data_arr.append(train_data_objs[key])\n",
    "\n",
    "for key in val_data_objs.keys():\n",
    "\n",
    "    val_data_arr.append(val_data_objs[key])\n",
    "\n",
    "for key in test_data_objs.keys():\n",
    "\n",
    "    test_data_arr.append(test_data_objs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202481c2-d347-448c-9399-e655adf4cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointTransformerModel(torch.nn.Module):\n",
    "\n",
    "    #------------Model architecture--------------\n",
    "    def __init__(self, num_classes=4, bbox_input_size=7):\n",
    "        \n",
    "        super(PointTransformerModel, self).__init__()\n",
    "        \n",
    "        #--------------Point Transformer layers----------------\n",
    "        self.conv1 = PointTransformerConv(in_channels=1, out_channels=10)\n",
    "        self.conv2 = PointTransformerConv(in_channels=10, out_channels=20)\n",
    "        self.conv3 = PointTransformerConv(in_channels=20, out_channels=30)\n",
    "\n",
    "        #-------Fully connected layers for the bounding box features-------\n",
    "        self.fc_bbox = torch.nn.Sequential(\n",
    "            torch.nn.Linear(bbox_input_size, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, 20)\n",
    "        )\n",
    "\n",
    "        #-------Final layers that take in the concatenated point cloud features and bounding box features------\n",
    "        #------Note: don't apply softmax at the end because when we use the cross entropy loss function that does \n",
    "        #------softmax in the code, so leave raw logits as output instead\n",
    "        self.fc_final = torch.nn.Sequential(\n",
    "            torch.nn.Linear(50, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            #torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(10, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, pos, batch = data.x, data.pos, data.batch\n",
    "        edge_index = knn_graph(pos, k=5, batch=batch, loop=False)\n",
    "        \n",
    "        x = F.relu(self.conv1(x, pos, edge_index))\n",
    "        x = F.relu(self.conv2(x, pos, edge_index))\n",
    "        x = F.relu(self.conv3(x, pos, edge_index))\n",
    "\n",
    "        x = global_max_pool(x, batch)\n",
    "\n",
    "        bbox_features = data.bbox_features\n",
    "        if bbox_features.dim() == 1:\n",
    "            bbox_features = bbox_features.unsqueeze(0)\n",
    "            \n",
    "        bbox_out = self.fc_bbox(bbox_features)\n",
    "\n",
    "        #-------Concatenate learned point cloud features and bounding box features-------- \n",
    "        x = torch.cat([x, bbox_out], dim=1)\n",
    "        \n",
    "        x = self.fc_final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def train_model(self, train_data, val_data, epochs=10, lr=0.001, weight_decay=1e-4):\n",
    "\n",
    "        #-----------Set optimizer and loss function-----------\n",
    "        optimizer = Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "        \n",
    "            self.train()  # Set model to training mode\n",
    "            total_loss = 0 ; correct = 0 ; total = 0\n",
    "            \n",
    "            # Batch loop with progress bar\n",
    "            pbar = tqdm(train_data, desc=f'Epoch {epoch}/{epochs}')\n",
    "            for data in pbar:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                # Forward pass\n",
    "                outputs = self.forward(data)\n",
    "                labels = data.y.view(-1)\n",
    "        \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item() * data.num_graphs  # Accumulate loss\n",
    "        \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                # Compute training accuracy\n",
    "                _, predicted = outputs.max(dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                train_acc = correct / total\n",
    "        \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix(loss=loss.item(), acc=train_acc)\n",
    "    \n",
    "        avg_loss = total_loss / size_train\n",
    "        print(f'Epoch {epoch}, Training Loss: {avg_loss:.4f}, Training Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "        # Validation phase\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        val_correct = 0; val_total = 0; val_loss = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                outputs = self.forward(data)\n",
    "                labels = data.y.view(-1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * data.num_graphs\n",
    "    \n",
    "                _, predicted = outputs.max(dim=1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss = val_loss / size_val\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "    \n",
    "        print('Training complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe187541-9c57-471a-a1a7-73ef04e693ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "size_train = len(train_data_arr)\n",
    "size_val = len(val_data_arr)\n",
    "size_test = len(test_data_arr)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(train_data_arr, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data_arr, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6636ae3-e8be-4ad4-9e0c-8d90890dbc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointTransformerModel(num_classes=4, bbox_input_size=7)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edcf9158-a399-4cb2-8ea7-eafcfd3bd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                                                                                                                       | 0/27 [00:00<?, ?it/s]/home/ubuntu/.local/lib/python3.9/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 32.45it/s, acc=0.326, loss=0.891]\n",
      "Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 51.75it/s, acc=0.504, loss=0.865]\n",
      "Epoch 3/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 51.95it/s, acc=0.526, loss=0.926]\n",
      "Epoch 4/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 52.04it/s, acc=0.548, loss=0.882]\n",
      "Epoch 5/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 51.78it/s, acc=0.622, loss=0.803]\n",
      "Epoch 6/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 51.51it/s, acc=0.611, loss=0.952]\n",
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 51.11it/s, acc=0.637, loss=0.82]\n",
      "Epoch 8/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 51.24it/s, acc=0.659, loss=0.834]\n",
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 51.23it/s, acc=0.767, loss=1.46]\n",
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 50.96it/s, acc=0.778, loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training Loss: 0.7439, Training Accuracy: 0.7778\n",
      "Validation Loss: 0.8707, Validation Accuracy: 0.6333\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model.train_model(train_loader, val_loader, epochs=epochs, lr=lr, weight_decay=weight_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
